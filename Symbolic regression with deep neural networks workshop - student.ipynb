{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpUMqXcyb0RL"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/benmoseley/symbolic-regression-with-deep-neural-networks-workshop/blob/main/Symbolic%20regression%20with%20deep%20neural%20networks%20workshop%20-%20student.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "# Symbolic regression with deep neural networks\n",
        "\n",
        "Written by Ben Moseley, April 2025.\n",
        "\n",
        "In this workshop we will train **deep neural networks** to carry out **symbolic regression**. This workshop is inspired by these papers: [Deep Learning for Symbolic Mathematics, Lample and Charton, 2019](https://arxiv.org/abs/1912.01412) and [End-to-end symbolic regression with transformers, Kamienny et al, 2022](https://arxiv.org/abs/2204.10532).\n",
        "\n",
        "The workshop is split into 3 parts:\n",
        " - Part 1: Understanding mathematical expressions and how to generate them\n",
        " - Part 2: Generating a training dataset of expressions and tokenization\n",
        " - Part 3: Training a deep neural network to carry out symbolic regression\n",
        "\n",
        "# Assessment\n",
        "\n",
        "You will be assessed on **your completion of each task in this notebook**. Each task is labelled like this:\n",
        " > **Task 0.0**: Complete this!\n",
        "\n",
        " Please fill in your code in the cell provided below each task.\n",
        "\n",
        "\n",
        "# Goal\n",
        "\n",
        "The goal of this workshop is to train a network to identify a mathematical expression given example data points from the expression. More precisely, given a dataset, $\\mathscr{D} = \\{(x_0, f(x_0)), \\dots, (x_N, f(x_N))\\}$, consisting of $N$ observed data points from a unknown function $f(x)$, can we predict the underlying **mathematical expression** for $f(x)$?\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-task.png\" width=\"60%\"></div>\n",
        "\n",
        "Note:\n",
        "- We are not just doing function fitting! In function fitting, we want to predict the **value** of $f$ given $\\mathscr{D}$. In symbolic regression we want to discover the underlying **mathematical expression** of $f$ given $\\mathscr{D}$. This means the network must predict an **expression**.\n",
        "- For simplicity, we only consider **scalar** (1D) functions with a single **input variable** ($x$) and **integer** constant coefficients. It is a fun task to extend the methods in this notebook to higher dimensions with floating point constants!\n",
        "\n",
        "# Approach\n",
        "\n",
        "We will use a deep neural network to **directly predict $f(x)$ given $\\mathscr{D}$ as input to the network**, as shown below:\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-train.png\" width=\"80%\"></div>\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-infer.png\" width=\"80%\"></div>\n",
        "\n",
        "In Part 2 we will explain how to enable the network to directly predict symbolic expressions by using **tokenization** and **autoregressive prediction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2KpP02Y1K8"
      },
      "source": [
        "# Part 1: Understanding mathematical expressions and how to generate them\n",
        "\n",
        "Our first step is to understand how to define and (randomly) generate mathematical expressions in Python. This code will be necessary in Part 2 to generate a training dataset of expressions for training the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXVST1MY5aF"
      },
      "source": [
        "## Part 1a: Defining mathematical expressions in Python\n",
        "\n",
        "One useful way to represent a mathematical expression is through a **tree** consisting of:\n",
        "- Nodes, which are either:\n",
        "    - Binary operations: e.g. $+, -, /$, or\n",
        "    - Unary operations: e.g. $\\sin, \\cos, \\exp, \\text{sqr}, \\text{abs}$, and\n",
        "- Leaves, which are\n",
        "    - Input variables, e.g. $x, y, z$ (note in this case we just consider $x$, the 1D case).\n",
        "\n",
        "For example, the expression $f(x) = \\sin(x + x^2)$ can be represented as the following tree:\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-tree.png\" width=\"40%\"></div>\n",
        "\n",
        "\n",
        "We will now randomly generate an expression tree in Python using two steps:\n",
        "\n",
        "1. Create a random binary tree consisting of only binary operations and leaves,\n",
        "2. Randomly add unary operations to the nodes of the tree.\n",
        "\n",
        "> **Task 1.1**: Complete the `generate_random_binary_tree` function so that given a list of binary operators, a list of tree leaves (input variables), and the total number nodes in the tree, it generates a random binary tree of only **binary operations**. Use `print_tree` to print out some generated trees.\n",
        "\n",
        "> HINT: a good way to work with trees is to use **recursion** (i.e. functions which call themselves) to iteratively build and traverse the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCTw4WrFdOBp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import json\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neaEt4ZacE4T"
      },
      "outputs": [],
      "source": [
        "# Define available binary operators and leaf symbols\n",
        "binary_operators = {\n",
        "    \"+\": lambda l, r: l + r,\n",
        "    \"-\": lambda l, r: l - r,\n",
        "    \"*\": lambda l, r: l * r,\n",
        "}\n",
        "leaves = (\"x\",)\n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    \"\"\"A node in a binary tree representing a mathematical expression.\n",
        "\n",
        "    Attributes:\n",
        "        val (str): The operator value at this node.\n",
        "        left (TreeNode or str): The left child (subtree or leaf).\n",
        "        right (TreeNode or str): The right child (subtree or leaf).\n",
        "    \"\"\"\n",
        "    def __init__(self, val=None, left=None, right=None):\n",
        "        self.val = val      # Node value: operator\n",
        "        self.left = left    # Left child node or leaf\n",
        "        self.right = right  # Right child node or leaf\n",
        "\n",
        "\n",
        "def generate_random_binary_tree(n_operators, binary_operators, leaves):\n",
        "    \"\"\"Generates a random expression tree consisting of binary operators.\n",
        "\n",
        "    Args:\n",
        "        n_operators (int): Number of binary operators to include in the tree.\n",
        "        binary_operators (dict): Mapping from binary symbols to binary functions.\n",
        "        leaves (tuple of str): Set of allowable leaf values (input variables).\n",
        "\n",
        "    Returns:\n",
        "        TreeNode: The root of the randomly generated expression tree.\n",
        "    \"\"\"\n",
        "    # Create a new root node with a random operator\n",
        "    root = TreeNode()\n",
        "    root.val = random.choice(list(binary_operators.keys()))\n",
        "    n_operators -= 1\n",
        "\n",
        "    # Randomly split the remaining operators between left and right subtrees\n",
        "    left_n_operators = random.randint(0, n_operators)\n",
        "    right_n_operators = n_operators - left_n_operators\n",
        "\n",
        "\n",
        "    ## TODO: assign `root.left` and `root.right` attributes\n",
        "\n",
        "\n",
        "\n",
        "    ##\n",
        "\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "def str_tree(root, level=0, prefix=\"Root \"):\n",
        "    \"\"\"Returns a pretty string representation of a TreeNode expression tree.\n",
        "\n",
        "    Args:\n",
        "        root (TreeNode or str): The current node or leaf.\n",
        "        level (int): The current depth in the tree (for indentation).\n",
        "        prefix (str): Prefix label for the node (e.g. 'Root ', 'L-- ').\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string representing the tree.\n",
        "    \"\"\"\n",
        "    indent = \" \" * (level * 4)\n",
        "    if isinstance(root, TreeNode):\n",
        "        s = indent + prefix + str(root.val) + \"\\n\"\n",
        "        s += str_tree(root.left, level + 1, \"L-- \")\n",
        "        s += str_tree(root.right, level + 1, \"R-- \")\n",
        "        return s\n",
        "    else:  # Leaf node\n",
        "        return indent + prefix + str(root) + \"\\n\"\n",
        "\n",
        "\n",
        "def print_tree(root):\n",
        "    \"\"\"Pretty-prints the tree structure.\"\"\"\n",
        "    print(str_tree(root))\n",
        "\n",
        "\n",
        "random.seed(123)\n",
        "for _ in range(5):\n",
        "    root = generate_random_binary_tree(n_operators=4, binary_operators=binary_operators, leaves=leaves)\n",
        "    print_tree(root)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEBu25wpKE-V"
      },
      "source": [
        "> **Task 1.2**: Complete the `add_random_unary_operators` function so that, given a binary tree (represented by a tree of `TreeNode`s), a list of unary operators, and the total number unary operators to add, the function randomly inserts unary operators into the tree.\n",
        "\n",
        "> HINT: we can simply treat unary operators as binary operators with a `TreeNode.right = None` attribute set to `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7aAnX6jImhk"
      },
      "outputs": [],
      "source": [
        "# Define available unary operators\n",
        "unary_operators = {\n",
        "    \"abs\": lambda o: sp.Abs(o),\n",
        "    \"sin\": lambda o: sp.sin(o),\n",
        "    \"tan\": lambda o: sp.tan(o),\n",
        "    \"exp\": lambda o: sp.exp(o),\n",
        "}\n",
        "\n",
        "\n",
        "def add_random_unary_operators(root, n_operators, unary_operators):\n",
        "    \"\"\"Adds unary operations randomly to internal nodes of a binary expression tree.\n",
        "\n",
        "    Args:\n",
        "        root (TreeNode): The root of the binary expression tree.\n",
        "        n_operators (int): Number of unary operations to insert.\n",
        "        unary_operators (dict): Mapping from unary symbols to unary functions.\n",
        "\n",
        "    Returns:\n",
        "        TreeNode: The root of the updated expression tree.\n",
        "    \"\"\"\n",
        "    if not isinstance(root, TreeNode):\n",
        "        raise Exception(f\"root is not a TreeNode, {root}\")\n",
        "\n",
        "    # Collect all internal (non-leaf) nodes into a flat list\n",
        "    nodes = []\n",
        "    def collect_nodes(node):\n",
        "        if isinstance(node, TreeNode):\n",
        "            nodes.append(node)\n",
        "            collect_nodes(node.left)\n",
        "            collect_nodes(node.right)\n",
        "    collect_nodes(root)\n",
        "\n",
        "    # Randomly add unary operations\n",
        "    for _ in range(n_operators):\n",
        "\n",
        "        # Randomly choose a node to wrap in a unary operator\n",
        "        chosen_node = random.choice(nodes)\n",
        "\n",
        "\n",
        "        ## TODO: wrap the chosen in a new TreeNode, `new_node`.\n",
        "        ## The new node should have a unary value, a left attribute as the chosen node, and `None` as the right attribute\n",
        "\n",
        "\n",
        "\n",
        "        ##\n",
        "\n",
        "\n",
        "        # Update references to the chosen node in the tree\n",
        "        if root is chosen_node:\n",
        "            root = new_node\n",
        "        else:\n",
        "            for node in nodes:  # Update in-place references\n",
        "                if node.left is chosen_node:\n",
        "                    node.left = new_node\n",
        "                elif node.right is chosen_node:\n",
        "                    node.right = new_node\n",
        "\n",
        "        # Add the new unary node to the list for possible further wrapping\n",
        "        nodes.append(new_node)\n",
        "\n",
        "    return root\n",
        "\n",
        "\n",
        "random.seed(123)\n",
        "for _ in range(3):\n",
        "    root = generate_random_binary_tree(n_operators=2, binary_operators=binary_operators, leaves=leaves)\n",
        "    print(\"Before unary operations added:\")\n",
        "    print_tree(root)\n",
        "    root = add_random_unary_operators(root, n_operators=3, unary_operators=unary_operators)\n",
        "    print(\"After unary operations added:\")\n",
        "    print_tree(root)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsmNi5m2mo4B"
      },
      "source": [
        "## Part 1b: using SymPy to display expressions\n",
        "\n",
        "Great! Now we are able to generate random mathematical expressions. Wouldn't it be nice to display the actual mathematical expression, rather than print out the tree? Fortunately, we can use [`sympy`](https://docs.sympy.org/latest/index.html) to help us!\n",
        "\n",
        "`sympy` is a Python library for symbolic mathematics. It allows us to directly manipulate mathematical expressions in Python, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB4Ku2jsNcYL"
      },
      "outputs": [],
      "source": [
        "# Define a symbolic variable\n",
        "x = sp.Symbol(\"x\")\n",
        "\n",
        "# Construct a symbolic expression using unary and binary operations\n",
        "f = sp.Abs(x**2 + sp.sin(x + x))\n",
        "\n",
        "# Display the symbolic expression\n",
        "f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba2QCLKn5tS"
      },
      "source": [
        "Note how `sympy` doesn't compute the value of $f$, but rather holds on to the expression itself. Also note how `sympy` automatically simplified the expression for us.\n",
        "\n",
        "### Evaluating mathematical expressions with `sympy`\n",
        "\n",
        "We can efficiently evaluate the expression given many values of `x` by using `sp.lambdify`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuedGmJknyEJ"
      },
      "outputs": [],
      "source": [
        "# Convert the symbolic expression `f` to a numerical function using NumPy\n",
        "fn = sp.lambdify(args=x, expr=f, modules=\"numpy\")\n",
        "\n",
        "# Generate input values and evaluate the function\n",
        "x_values = np.linspace(-1, 1, 100)  # 100 points between -1 and 1\n",
        "f_values = fn(x_values)  # Evaluate the function on the input values\n",
        "\n",
        "# Print shapes for verification\n",
        "print(x_values.shape, f_values.shape)\n",
        "\n",
        "# Plot the evaluated function\n",
        "plt.figure()\n",
        "plt.scatter(x_values, f_values)\n",
        "plt.title(f)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4l6TiU8qqZE"
      },
      "source": [
        "Ok, now we are ready to convert our Python trees to `sympy` expressions.\n",
        "\n",
        "> **Task 1.3**: complete the `tree_to_sympy` function, which converts a tree of `TreeNode`s to a `sympy` expression.\n",
        "\n",
        "> HINT: use recursion again to help build the expression up from its leaves!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3imhQuCsaSn"
      },
      "outputs": [],
      "source": [
        "def tree_to_sympy(root, binary_operators, unary_operators, leaves):\n",
        "    \"\"\"Converts a binary tree expression into a SymPy expression.\n",
        "\n",
        "    Args:\n",
        "        root (TreeNode): The root of the binary expression tree.\n",
        "        binary_operators (dict): Mapping from binary symbols to binary functions.\n",
        "        unary_operators (dict): Mapping from unary symbols to unary functions.\n",
        "        leaves (tuple of str): Set of allowable leaf values (input variables).\n",
        "\n",
        "    Returns:\n",
        "        sympy.Expr: The SymPy expression corresponding to the tree.\n",
        "    \"\"\"\n",
        "    if isinstance(root, TreeNode):\n",
        "        if root.val in binary_operators:\n",
        "\n",
        "\n",
        "            ## TODO: complete building the sympy expression for a binary operator.\n",
        "            ## Recursively build left and right `sympy` expressions, then evaluate\n",
        "            ## and return the binary `sympy` expression.\n",
        "\n",
        "            return TODO\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "        elif root.val in unary_operators:\n",
        "\n",
        "\n",
        "            ## TODO: complete building the sympy expression for a unary operator.\n",
        "            ## Recursively build left `sympy` expression, then evaluate and return\n",
        "            ## the unary `sympy` expression.\n",
        "\n",
        "            return TODO\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "        else:\n",
        "            raise Exception(f\"root value not recognised: {root.val}\")  # Unrecognized operator\n",
        "    else:  # leaf node\n",
        "        assert root in leaves  # Ensure the root is a valid leaf value\n",
        "        expr = sp.Symbol(root)  # Convert leaf value to a SymPy symbol\n",
        "        return expr\n",
        "\n",
        "\n",
        "# Example tree: (x * x) + sin(x + x)\n",
        "root = TreeNode(\"+\", TreeNode(\"*\", \"x\", \"x\"), TreeNode(\"sin\", TreeNode(\"+\", \"x\", \"x\"), None))\n",
        "print_tree(root)\n",
        "\n",
        "# Convert the tree to a SymPy expression and verify the result\n",
        "tree_to_sympy(root, binary_operators, unary_operators, leaves)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0T69Arxu4T"
      },
      "source": [
        "# Part 2: Generating a training dataset of expressions and tokenization\n",
        "\n",
        "In this part we will generate a large dataset of expressions using the code above. We will also **filter** and **tokenize** the expressions in the dataset to make them suitable for machine learning. We will use this dataset to train a neural network to carry out symbolic regression in Part 3.\n",
        "\n",
        "Remember our goal is to train a network so that given the input $\\mathscr{D} = \\{(x_0, f(x_0)), \\dots, (x_N, f(x_N))\\}$, it predicts the expression for $f(x)$.\n",
        "\n",
        "Thus, we need to generate many examples of $\\mathscr{D}$ and $f$ as training data. We define the training dataset so that **each example** in the dataset consists of a **randomly generated $f$ (using the code above) and its corresponding observations, $\\mathscr{D}$**, which we will compute using `sympy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKMQQl-ZIVr"
      },
      "source": [
        "## Part 2a: Generating many random expressions\n",
        "\n",
        "\n",
        "Our first step is to generate many random expressions using the code above.\n",
        "\n",
        "\n",
        "> **Task 2.1**: use `generate_random_binary_tree`, `add_random_unary_operators`, and `tree_to_sympy` to generate 100,000 random expressions.\n",
        "\n",
        "> HINT: if you were unable to complete these functions in Part 1, you can skip this task and just load some pre-computed trees using the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6BzhBudz45P"
      },
      "outputs": [],
      "source": [
        "# Maximum number of binary and unary operators for tree generation\n",
        "max_n_binary_operators = 4\n",
        "max_n_unary_operators = 3\n",
        "\n",
        "# Lists to store the generated tree roots and corresponding SymPy expressions\n",
        "roots = []\n",
        "exprs = []\n",
        "\n",
        "random.seed(123)\n",
        "for i in range(100000):  # Generate 100,000 trees (should take 1-2 mins)\n",
        "\n",
        "    # Randomly sample the size of the tree\n",
        "    n_binary_operators = random.randint(1, max_n_binary_operators)\n",
        "    n_unary_operators = random.randint(1, max_n_unary_operators)\n",
        "\n",
        "\n",
        "    ## TODO: generate a random tree and append it to `roots`.\n",
        "    ## Also convert the tree to its `sympy` expression and append it to `exprs`.\n",
        "\n",
        "\n",
        "\n",
        "    ##\n",
        "\n",
        "\n",
        "# Print the lengths of the generated trees and expressions\n",
        "print(len(roots), len(exprs))\n",
        "\n",
        "# Print the first few expressions for verification\n",
        "print(exprs[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2nre5de5oAl"
      },
      "outputs": [],
      "source": [
        "## OPTIONAL: Use this code to skip Task 2.1 and load pre-computed `roots` and `exprs`\n",
        "\n",
        "def load_trees(filename=\"trees.json\"):\n",
        "    \"\"\"Loads pre-computed trees and their corresponding SymPy expressions from a compressed JSON file.\"\"\"\n",
        "    with gzip.open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert the tree dictionaries back into TreeNode objects and parse expressions\n",
        "    roots = [dict_to_tree(d) for d in data[\"trees\"]]\n",
        "    exprs = [sp.sympify(s, evaluate=False) for s in data[\"exprs\"]]  # Parse expressions as SymPy objects\n",
        "    return roots, exprs\n",
        "\n",
        "\n",
        "def dict_to_tree(root):\n",
        "    \"\"\"Converts a dictionary representation of a tree back into a TreeNode object.\"\"\"\n",
        "    if isinstance(root, dict):\n",
        "        return TreeNode(\n",
        "            val=root[\"val\"],\n",
        "            left=dict_to_tree(root[\"left\"]),\n",
        "            right=dict_to_tree(root[\"right\"]),\n",
        "        )\n",
        "    else:\n",
        "        return root  # Leaf node value\n",
        "\n",
        "# Load pre-computed `roots` and `exprs`\n",
        "roots, exprs = load_trees()\n",
        "\n",
        "# Print the length of the loaded trees and expressions\n",
        "print(len(roots), len(exprs))\n",
        "\n",
        "# Print the first few expressions for verification\n",
        "print(exprs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBkj9Nsy7DRJ"
      },
      "source": [
        "Next, we need to evaluate $\\mathscr{D} = \\{(x_0, f(x_0)), \\dots, (x_N, f(x_N))\\}$ for each of these functions. For simplicity, we will use the **same** $x_i$ values for all expressions, sampled regularly over the range $[-1,1]$, with $N=100$, i.e. `x_values = np.linspace(-1,1,100)`.\n",
        "\n",
        "> **Task 2.2**: complete the code below to generate $\\mathscr{D}$ for each expression.\n",
        "\n",
        "> HINT: you can use `sp.lambdify` to evaluate $f$ as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUoXOYrdAgVL"
      },
      "outputs": [],
      "source": [
        "# Values to evaluate each expression over\n",
        "x_values = np.linspace(-1, 1, 100)\n",
        "\n",
        "# List to store the observational data for each expression\n",
        "datas = []\n",
        "\n",
        "for expr in exprs:\n",
        "\n",
        "\n",
        "    ## TODO: Compute observational `data` for each expression and append it to `datas`.\n",
        "    ## `data` should be a `numpy` array of shape (N, 2), where the first column contains\n",
        "    ## `x_values`, and the second column contains `f(x_values)`.\n",
        "\n",
        "\n",
        "\n",
        "    ##\n",
        "\n",
        "\n",
        "    # Ensure that the data is a numpy array with shape (100, 2)\n",
        "    assert isinstance(data, np.ndarray) and datas[-1].shape == (100, 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGn1JmdqFAaa"
      },
      "source": [
        "## Part 2b: Filtering the dataset\n",
        "\n",
        "As the dataset was being generated, you may have noticed two things:\n",
        "1. some of the expressions are quite extreme (such as $f=\\exp(\\exp(\\exp(x)))$), and `sp.lambdify` emits overflow `RuntimeWarning`s when computing the value of $f$.\n",
        "2. by random chance, many of the expressions generated have the same tree structure (i.e. are the same expression).\n",
        "\n",
        "So, next we will filter our training dataset to:\n",
        "1. remove extreme expressions, and\n",
        "2. remove duplicate trees.\n",
        "\n",
        "> **Task 2.3**: Filter out expressions where the absolute value of any $f(x_i)$ in $\\mathscr{D}$ is greater than 10. Also filter out all duplicate trees from the dataset.\n",
        "\n",
        "> HINT: you can check for duplicates by using `str_tree` to serialise each tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x562Y62H9us"
      },
      "outputs": [],
      "source": [
        "# Lists to store the filtered roots, expressions, and corresponding data\n",
        "roots_filtered = []\n",
        "exprs_filtered = []\n",
        "datas_filtered = []\n",
        "roots_unique = []  # To keep track of unique tree structures (to filter duplicates)\n",
        "\n",
        "# Loop through all the generated trees, expressions, and their observational data\n",
        "for root, expr, data in zip(roots, exprs, datas):\n",
        "\n",
        "\n",
        "    ## TODO: Filter out extreme expressions and duplicate trees from `roots`, `exprs`, and `datas`.\n",
        "\n",
        "\n",
        "\n",
        "    ##\n",
        "\n",
        "\n",
        "# Print the number of trees remaining after filtering, and how many were removed\n",
        "print(f\"Total number of trees in filtered dataset: {len(roots_filtered)} ({len(roots)-len(roots_filtered)} trees removed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyH7-Gi4H-Tl"
      },
      "source": [
        "Finally, now we have generated and filtered the dataset, let's plot a few examples in the dataset look reasonable.\n",
        "\n",
        "> **Task 2.4:** Plot the observational data for a few example expressions in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aKeRtgCITYk"
      },
      "outputs": [],
      "source": [
        "## TODO: Plot a few examples in the filtered dataset.\n",
        "\n",
        "\n",
        "\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqkW91i1A-NU"
      },
      "source": [
        "## Part 2c: Tokenization\n",
        "\n",
        "Neural networks fundamentally process **numbers** (stored as **vectors**). But we want our network to directly predict a **mathematical expression**. Therefore, we need a way to represent a mathematical expression as a number (or sequence of numbers), so that the network can manipulate and predict it. We also need a way to convert back and forth between expressions and their numerical representation, so that we can read out the predicted expressions.\n",
        "\n",
        "The process of converting non-vector inputs, such as text or symbols, into numerical vectors suitable for machine learning is known as **tokenization**. What we need is a **unique mapping** that allows us to go from expressions to vectors (tokens) and from vectors (tokens) back to expressions. The simplest way to do this is to define a **dictionary**, where all expression symbols are assigned a unique integer index. In our case, we will use the mapping:\n",
        "\n",
        "| Symbol    | Token |\n",
        "|-----------|-------|\n",
        "| `+`       | 0     |\n",
        "| `-`       | 1     |\n",
        "| `*`       | 2     |\n",
        "| `abs`     | 3     |\n",
        "| `sin`     | 4     |\n",
        "| `tan`     | 5     |\n",
        "| `exp`     | 6     |\n",
        "| `x`       | 7     |\n",
        "| `None`    | 8     |\n",
        "| `<EOS>`   | 9     |\n",
        "| `<PAD>`   | 10    |\n",
        "\n",
        "`<EOS>` and `<PAD>` are special \"end of sequence\" and \"padding\" symbols we added which will be useful for training our neural network and will be described in Part 3.\n",
        "\n",
        "Now, given an expression tree, such as\n",
        "\n",
        "<pre>Root sin\n",
        "    L-- *\n",
        "        L-- x\n",
        "        R-- x\n",
        "    R-- None</pre>\n",
        "\n",
        "We can go through and replace each symbol with its token:\n",
        "\n",
        "<pre>Root 4\n",
        "    L-- 2\n",
        "        L-- 7\n",
        "        R-- 7\n",
        "    R-- 8</pre>\n",
        "\n",
        "Finally, we can turn this into a vector by **flattening** the tree. There are many possible ways to flatten trees; in this case we will use **prefix notation**, writing each node before its children, listed from left to right:\n",
        "\n",
        "``[4, 2, 7, 7, 8]``\n",
        "\n",
        "The task of predicting $f$ thus turns in to the task of predicting a **sequence of integers**. This can be thought of as a **classification task**, and it allows us to use a **cross-entropy loss function** to train our network (very similiar to how modern LLMs are trained).\n",
        "\n",
        "The code below:\n",
        "1. defines our tokenization maps\n",
        "2. defines `tree_to_prefix_sequence` and `prefix_sequence_to_tree` functions\n",
        "3. computes a list of vectors, `vecs_filtered`, for the training dataset above.\n",
        "\n",
        "> **Task 2.5**: run the code below, and make sure you understand what each line is doing.\n",
        "\n",
        "> **Task 2.6**: plot a histogram showing the count of each token over the entire training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vRQO43eqM6_"
      },
      "outputs": [],
      "source": [
        "def create_token_maps(binary_operators, unary_operators, leaves):\n",
        "    \"\"\"\n",
        "    Creates forward and backward token maps.\n",
        "\n",
        "    Args:\n",
        "        binary_operators (dict): Mapping from binary symbols to binary functions.\n",
        "        unary_operators (dict): Mapping from unary symbols to unary functions.\n",
        "        leaves (tuple of str): Set of allowable leaf values (input variables).\n",
        "\n",
        "    Returns:\n",
        "        forward_token_map (dict): Mapping from symbol to token index.\n",
        "        backward_token_map (dict): Mapping from token index to symbol.\n",
        "    \"\"\"\n",
        "    # Combine all tokens (binary operators, unary operators, leaves, special tokens)\n",
        "    all_tokens = list(binary_operators.keys()) + list(unary_operators.keys()) + list(leaves) + [None,] + [\"<EOS>\"] + [\"<PAD>\"]\n",
        "\n",
        "    # Create forward and backward token maps\n",
        "    return ({token: idx for idx, token in enumerate(all_tokens)},\n",
        "            {idx: token for idx, token in enumerate(all_tokens)})\n",
        "\n",
        "def tree_to_prefix_sequence(root):\n",
        "    \"\"\"\n",
        "    Converts a binary tree to a prefix sequence.\n",
        "\n",
        "    Args:\n",
        "        root (TreeNode): The root node of the tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list representing the prefix sequence of the tree.\n",
        "    \"\"\"\n",
        "    if isinstance(root, TreeNode):\n",
        "        return [root.val] + tree_to_prefix_sequence(root.left) + tree_to_prefix_sequence(root.right)\n",
        "    else:  # Leaf node\n",
        "        return [root]\n",
        "\n",
        "def prefix_sequence_to_tree(sequence, binary_operators, unary_operators, leaves):\n",
        "    \"\"\"\n",
        "    Converts a prefix sequence back to a binary tree.\n",
        "\n",
        "    Args:\n",
        "        sequence (list): The prefix sequence.\n",
        "        binary_operators (dict): Mapping from binary symbols to binary functions.\n",
        "        unary_operators (dict): Mapping from unary symbols to unary functions.\n",
        "        leaves (tuple of str): Set of allowable leaf values (input variables).\n",
        "\n",
        "    Returns:\n",
        "        root (TreeNode): The reconstructed binary tree root.\n",
        "    \"\"\"\n",
        "    def build_binary_tree(iterator):\n",
        "        val = next(iterator)\n",
        "        root = TreeNode()\n",
        "        root.val = val\n",
        "\n",
        "        # If the value is an operator, recursively build left and right subtrees\n",
        "        if val in binary_operators or val in unary_operators:\n",
        "            count_left, root.left = build_binary_tree(iterator)\n",
        "            count_right, root.right = build_binary_tree(iterator)\n",
        "            if val in unary_operators:\n",
        "                assert root.right is None  # Unary operators should not have a right child\n",
        "        else:\n",
        "            # If it's a leaf node, return the value\n",
        "            assert val in leaves + (None,)  # Check consistency\n",
        "            return 1, val  # Leaf node\n",
        "\n",
        "        return 1 + count_left + count_right, root\n",
        "\n",
        "    count, root = build_binary_tree(iter(sequence))\n",
        "    assert count == len(sequence)  # Ensure the number of nodes matches the sequence length\n",
        "    return root\n",
        "\n",
        "# Create forward and backward token maps for the vocabulary\n",
        "forward_token_map, backward_token_map = create_token_maps(binary_operators, unary_operators, leaves)\n",
        "\n",
        "# Print token maps and vocabulary size\n",
        "print(f\"forward token map: {forward_token_map}\")\n",
        "print(f\"backward token map: {backward_token_map}\")\n",
        "n_tokens = len(forward_token_map)\n",
        "print(f\"Vocabulary size: {n_tokens}\")\n",
        "\n",
        "# Convert the filtered roots to prefix sequences and their corresponding vector representations\n",
        "seqs_filtered = [tree_to_prefix_sequence(root) for root in roots_filtered]\n",
        "vecs_filtered = [np.array([forward_token_map[s] for s in seq]) for seq in seqs_filtered]\n",
        "\n",
        "# Display information for the first two trees in the filtered dataset\n",
        "for root, seq, vec in zip(roots_filtered[:2], seqs_filtered, vecs_filtered):\n",
        "    print_tree(root)\n",
        "    print(f\"Flattened tree: {seq}\")\n",
        "    print(f\"Vector representation: {vec}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob1ZScRLuXS8"
      },
      "outputs": [],
      "source": [
        "## TODO: Plot a histogram showing the count of each token (symbol) over the entire training dataset.\n",
        "\n",
        "\n",
        "\n",
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQD6niRCBCqT"
      },
      "source": [
        "# Part 3: Training a deep neural network to carry out symbolic regression\n",
        "\n",
        "We are now ready to train a neural network to carry out symbolic regression.\n",
        "\n",
        "We will train the network to **iteratively** generate expressions. More specifically, during **training**, we will teach the network to perform **next token prediction** — given a partial tree sequence of tokens and $\\mathscr{D}$, it learns to predict the next token in the sequence.\n",
        "\n",
        "Then, during **inference**, we generate the full output using **autoregression** — given an empty starting sequence and $\\mathscr{D}$, we predict the next token, append it to the sequence, and repeat this process until an end-of-sequence token is predicted or a maximum sequence length is reached.\n",
        "\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-train.png\" width=\"80%\"></div>\n",
        "\n",
        "<div align=\"center\"><img src=\"https://benmoseley.blog/uploads/teaching/2025-AIMS-SA/symbolic-regression-infer.png\" width=\"80%\"></div>\n",
        "\n",
        "\n",
        "We will use the special end of sequence (`<EOS>`) token to allow the network to predict the end of a sequence, and the pad (`<PAD>`) token to randomly pad the front of each training example so that the network can learn to predict tokens at different parts of the tree.\n",
        "\n",
        "The network will be trained using a **cross-entropy loss** on next-token prediction. Mathematically, given a training sequence of tokens $(x_1, x_2, \\dots, x_T)$, the loss is defined as:\n",
        "\n",
        "$$\n",
        "\\mathscr{L} = - \\log(p(x_T | x_1, x_2, \\dots, x_{T-1}, \\mathscr{D}))\n",
        "$$\n",
        "\n",
        "where $p(x_T | x_1, x_2, \\dots, x_{T-1}, \\mathscr{D})$ is the probability assigned by the model to the correct next token $x_T$, given the previous token sequence of length `T - 1` and the input data observations $\\mathscr{D}$.\n",
        "\n",
        "The network predicts the class probability of the next token. Note we remove the `<PAD>` token from the final output classes, such that there are `n_tokens - 1` output classes.\n",
        "\n",
        "Overall, our approach trains the model to build expressions one token at a time while implicitly learning the structure and syntax of mathematical expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTmAsJ7eZQn6"
      },
      "source": [
        "## Part 3a: Defining a neural network for symbolic regression\n",
        "\n",
        "For simplicity, we will use a simple fully connected encoder-decoder architecture for our neural network.\n",
        "\n",
        "During training, the inputs and outputs of the model must have the following shapes:\n",
        "\n",
        "Inputs:\n",
        "- Batch of $\\mathscr{D}$: shape `(B, N, 2)`\n",
        "- Batch of previous token sequence: shape `(B, T - 1)`\n",
        "\n",
        "Outputs:\n",
        "- Batch of class probabilities for next token: shape (`B, n_tokens - 1`) (Note we remove the `<PAD>` token from the final output classes).\n",
        "\n",
        "Here `N` is the number of data observations, `T` is the maximum tree size, `B` is the batch size, and `n_tokens` is the number of tokens in the vocabulary.\n",
        "\n",
        "> **Task 3.1**: Our PyTorch model is defined below. Finish writing the `inference` function, which autoregressively predicts an expression sequence given `data`. Check that the `forward` and `inference` functions behave as you expect.\n",
        "\n",
        "> HINT: you can use `torch.argmax(logits, dim=1)` to get the most likely token predicted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGPz42EtjAxb"
      },
      "outputs": [],
      "source": [
        "class SeqFCN(nn.Module):\n",
        "    def __init__(self, N, T, n_tokens, d_model=128, n_enc_layers=2, n_dec_layers=2):\n",
        "        \"\"\"\n",
        "        Initialize the SeqFCN model.\n",
        "\n",
        "        Args:\n",
        "            N (int): Number of input data points.\n",
        "            T (int): Maximum length of the token sequence.\n",
        "            n_tokens (int): Size of the vocabulary.\n",
        "            d_model (int, optional): Dimension of the model's hidden state (default is 128).\n",
        "            n_enc_layers (int, optional): Number of layers in the encoder (default is 2).\n",
        "            n_dec_layers (int, optional): Number of layers in the decoder (default is 2).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.T = T\n",
        "        self.n_tokens = n_tokens\n",
        "\n",
        "        # Embedding for symbolic input\n",
        "        self.embed1 = nn.Embedding(n_tokens, 4)\n",
        "\n",
        "        # Encoder: input is flattened data values of shape (B, N*2)\n",
        "        enc_layers = [nn.Linear(N*2, d_model), nn.ReLU()]\n",
        "        for _ in range(n_enc_layers - 1):\n",
        "            enc_layers += [nn.Linear(d_model, d_model), nn.ReLU()]\n",
        "        self.encoder = nn.Sequential(*enc_layers)\n",
        "\n",
        "        # Decoder: input is encoder_out + embedded tree vector\n",
        "        input_dim = d_model + 4 * (T - 1)\n",
        "        dec_layers = [nn.Linear(input_dim, d_model), nn.ReLU()]\n",
        "        for _ in range(n_dec_layers - 1):\n",
        "            dec_layers += [nn.Linear(d_model, d_model), nn.ReLU()]\n",
        "        dec_layers += [nn.Linear(d_model, n_tokens - 1)]  # Don't predict padding token\n",
        "        self.decoder = nn.Sequential(*dec_layers)\n",
        "\n",
        "    def forward(self, data, x):\n",
        "        \"\"\"\n",
        "        Next token class prediction.\n",
        "\n",
        "        Args:\n",
        "            data: Tensor of shape (B, N, 2) — data observations\n",
        "            x: Tensor of shape (B, T - 1) — previous token sequence\n",
        "\n",
        "        Returns:\n",
        "            logits: Tensor of shape (B, n_tokens - 1) - class probabilities of next token (raw logits)\n",
        "        \"\"\"\n",
        "        # encode data\n",
        "        B = x.shape[0]\n",
        "        d = data.view(B, -1)         # Flatten input: (B, 2 * N)\n",
        "        d = self.encoder(d)          # (B, d_model)\n",
        "\n",
        "        # embed tokens\n",
        "        x = self.embed1(x)           # (B, T-1, 4)\n",
        "        x = x.view(B, -1)            # (B, 4 * (T - 1))\n",
        "\n",
        "        # decode class probabilities (raw logits)\n",
        "        x = torch.cat([d, x], dim=1) # (B, d_model + 4 * (T - 1))\n",
        "        logits = self.decoder(x)     # (B, n_tokens - 1)\n",
        "        return logits\n",
        "\n",
        "    def inference(self, data):\n",
        "        \"\"\"\n",
        "        Predict entire expression sequence autoregressively from `data`.\n",
        "\n",
        "        Args:\n",
        "            data: Tensor of shape (B, N, 2) — data observations\n",
        "\n",
        "        Returns:\n",
        "            x: Tensor of shape (B, T) - sequence of tokens\n",
        "        \"\"\"\n",
        "        B = data.shape[0]\n",
        "        x = torch.ones((B, self.T), dtype=torch.long) * forward_token_map[\"<PAD>\"]  # Initialize `x` with all pad tokens\n",
        "        for _ in range(self.T):  # Loop over entire sequence\n",
        "\n",
        "\n",
        "            ## TODO: autoregressively predict entire sequence\n",
        "\n",
        "\n",
        "            ##\n",
        "\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example initialization and check\n",
        "N = 100\n",
        "T = (2*(max_n_binary_operators + max_n_unary_operators) + 1) + 1  # max tree size in training dataset = max number of nodes + max number of leaves + EOS token\n",
        "print(f\"Maximum tree size (T): {T}\")\n",
        "print(f\"Vocabulary size (n_tokens): {n_tokens}\")\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = SeqFCN(N, T, n_tokens)\n",
        "print(model)\n",
        "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters()):.2e}\")\n",
        "\n",
        "# Test forward pass and inference\n",
        "data = torch.ones(200, N, 2)\n",
        "x = torch.ones(200, T - 1, dtype=torch.long)\n",
        "assert model(data, x).shape == (200, n_tokens - 1)\n",
        "assert model.inference(data).shape == (200, T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOchYe6BmIw"
      },
      "source": [
        "## Part 3b: Training the network\n",
        "\n",
        "Finally, we will train the model using our training dataset and a cross entropy loss function.\n",
        "\n",
        "Note when sampling batches of training examples, we apply random left-padding to each expression sequence, with up to `T - 1` `<PAD>` tokens, followed by right-padding with `<EOS>` tokens until each expression has the same maximum length of `T`.\n",
        "\n",
        "> **Task 3.2**: Use the code below to train the `SeqFCN` model. Improve the `test` function so that it also reports the accuracy of the model (number of expressions correctly guessed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFinRfYQEE3V"
      },
      "outputs": [],
      "source": [
        "# Define train/test datasets\n",
        "# Stack filtered data (B, N, 2)\n",
        "datas_ = np.stack(datas_filtered, axis=0)  # Shape: (B, N, 2)\n",
        "\n",
        "# Stack vector representations, padding with <EOS> token to max length T\n",
        "vecs_ = np.stack([np.concatenate([vec, forward_token_map[\"<EOS>\"] * np.ones(T - vec.shape[0], dtype=int)])  # Pad with <EOS>\n",
        "                 for vec in vecs_filtered], axis=0)  # Shape: (B, T)\n",
        "\n",
        "# Print shapes to verify correctness\n",
        "print(datas_.shape, vecs_.shape)\n",
        "\n",
        "# Split into train (80%) and test (20%)\n",
        "split = 8 * datas_.shape[0] // 10\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(datas_[:split], dtype=torch.float32),\n",
        "                                               torch.tensor(vecs_[:split], dtype=torch.long))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(datas_[split:], dtype=torch.float32),\n",
        "                                              torch.tensor(vecs_[split:], dtype=torch.long))\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "print(f\"Number of test examples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ55EId0BreI"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataset, test_dataset, batch_size=100, n_steps=50000, lr=1e-3):\n",
        "    \"\"\"\n",
        "    Trains the model on the given training dataset and evaluates it on both training and test datasets periodically.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train.\n",
        "        train_dataset: The dataset used for training.\n",
        "        test_dataset: The dataset used for testing.\n",
        "        batch_size: The number of samples per batch during training (default 100).\n",
        "        n_steps: The number of training steps (default 50000).\n",
        "        lr: The learning rate for the optimizer (default 1e-3).\n",
        "\n",
        "    Returns:\n",
        "        The trained model.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    # Create DataLoader for training dataset\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initial evaluation on train and test datasets\n",
        "    test(model, train_dataset, \"train\")\n",
        "    test(model, test_dataset, \"test\")\n",
        "\n",
        "    # Set the device for model (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    i = 0\n",
        "    while i < n_steps:\n",
        "        for batch_idx, (data_batch, vecs_batch) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            data_batch = data_batch.to(device)  # (B, N, 2)\n",
        "            vecs_batch = vecs_batch.to(device)  # (B, T)\n",
        "\n",
        "            # Randomly add padding to the front of vecs_batch, cropping back to max sequence length T\n",
        "            x = torch.arange(vecs_batch.shape[0]).unsqueeze(1).expand(vecs_batch.shape)\n",
        "            y = torch.arange(vecs_batch.shape[1]).unsqueeze(0).expand(vecs_batch.shape)\n",
        "            shifts = torch.randint(1, vecs_batch.shape[1], (vecs_batch.shape[0],))  # 1 to T-1 pad tokens at front\n",
        "            y = y + shifts.view(-1, 1)\n",
        "            padded = torch.cat([torch.full_like(vecs_batch, forward_token_map[\"<PAD>\"]), vecs_batch], axis=1)\n",
        "            vecs_batch = padded[x, y]  # (B, T)\n",
        "\n",
        "            # Compute loss and optimize model\n",
        "            logits = model(data_batch, vecs_batch[:, :-1])  # (B, n_tokens-1)\n",
        "            loss = criterion(logits, vecs_batch[:, -1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track and display progress\n",
        "            i += 1\n",
        "            if i % 1000 == 0 or i == 1:\n",
        "                print(f\"{i}/{n_steps}, {loss.item()}\")\n",
        "            if i % 5000 == 0:\n",
        "                test(model, train_dataset, \"train\")\n",
        "                test(model, test_dataset, \"test\")\n",
        "                model.to(device)\n",
        "                model.train()\n",
        "\n",
        "            if i >= n_steps:\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, dataset, tag):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataset and prints the accuracy.\n",
        "    Also plots some predictions vs. true expressions.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate.\n",
        "        dataset: The dataset to evaluate on.\n",
        "        tag: A string to label the dataset (e.g., \"train\", \"test\").\n",
        "    \"\"\"\n",
        "    model.to(torch.device(\"cpu\"))\n",
        "    model.eval()\n",
        "\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        # Collect predictions and targets from the dataset\n",
        "        for data_batch, vecs_batch in torch.utils.data.DataLoader(dataset, batch_size=500, shuffle=False):\n",
        "            pred = model.inference(data_batch)\n",
        "            preds.append(pred)\n",
        "            targets.append(vecs_batch)\n",
        "\n",
        "    # Concatenate predictions and targets\n",
        "    preds = torch.concat(preds)  # (B, T)\n",
        "    targets = torch.concat(targets)  # (B, T)\n",
        "\n",
        "\n",
        "    ## TODO: report the accuracy of the model (number of expressions correctly predicted)\n",
        "\n",
        "\n",
        "    ##\n",
        "\n",
        "\n",
        "    # Plot some expression predictions\n",
        "    nrows, ncols = 6, 6\n",
        "    plt.figure(figsize=(0.7 * np.array([ncols * 2.5, nrows * 2.5])))\n",
        "\n",
        "    for i in range(min(nrows * ncols, len(dataset))):\n",
        "        # Get symbol sequences from predicted vectors\n",
        "        seq_pred = [backward_token_map[t.item()] for t in preds[i]]\n",
        "        assert \"<PAD>\" not in seq_pred\n",
        "        seq_true = [backward_token_map[t.item()] for t in targets[i]]\n",
        "        assert \"<PAD>\" not in seq_true\n",
        "\n",
        "        # Cut sequence after first <EOS> token\n",
        "        try:\n",
        "            ix = seq_pred.index(\"<EOS>\")\n",
        "            seq_pred = seq_pred[:ix]\n",
        "        except: pass\n",
        "        try:\n",
        "            ix = seq_true.index(\"<EOS>\")\n",
        "            seq_true = seq_true[:ix]\n",
        "        except: pass\n",
        "\n",
        "        # Convert symbol sequence to tree and expression\n",
        "        try:\n",
        "            root_pred = prefix_sequence_to_tree(seq_pred, binary_operators, unary_operators, leaves)\n",
        "            expr_pred = tree_to_sympy(root_pred, binary_operators, unary_operators, leaves)\n",
        "        except:  # If not a valid expression\n",
        "            expr_pred = None\n",
        "\n",
        "        # True expression\n",
        "        root_true = prefix_sequence_to_tree(seq_true, binary_operators, unary_operators, leaves)\n",
        "        expr_true = tree_to_sympy(root_true, binary_operators, unary_operators, leaves)\n",
        "\n",
        "        # Plot the true and predicted expressions\n",
        "        def plot_expr(expr, label):\n",
        "            x = np.linspace(-1, 1, 100)\n",
        "            f = sp.lambdify(args=sp.symbols(leaves), expr=expr, modules=\"numpy\")(x)\n",
        "            if isinstance(f, (int, float)): f *= np.ones(100)\n",
        "            plt.plot(x, f, label=f\"{label} {expr}\")\n",
        "\n",
        "        plt.subplot(nrows, ncols, i + 1)\n",
        "        plot_expr(expr_true, \"true\")\n",
        "\n",
        "        if expr_pred is None:\n",
        "            plt.plot([0], [0], label=\"pred invalid expr\")\n",
        "        else:\n",
        "            plot_expr(expr_pred, \"pred\")\n",
        "\n",
        "        plt.legend(fontsize=\"xx-small\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Initialize the model\n",
        "torch.manual_seed(123)\n",
        "model = SeqFCN(N, T, n_tokens)\n",
        "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters()):.2e}\")\n",
        "\n",
        "# Train the model\n",
        "model = train(model, train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsRN9VuGD4lV"
      },
      "source": [
        "## Part 3c: Open-ended exploration and written report\n",
        "\n",
        "> **Task 3.3**: This is an open-ended task. Think of ways that the model and or training algorithm could be improved, and/or ways to better analyse the predictions of the model, and try these out. For example, you could investigate:\n",
        "- Comparisons of the performance of different hyperparameters, and other neural network architectures (e.g. Transformers)\n",
        "- Analysis of the generalisation / out-of-distribution performance of the network\n",
        "- Written thoughts on limitations and future extensions of your model\n",
        ">\n",
        "> Use the space below to write a **summary report** on your findings.\n",
        ">\n",
        "> The report can include text, figures, code snippets, and any other colab cell outputs you think are appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DseFVxsbGgTO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1eC7bPHO2zwy3P358fmveAe7eRTkGJtO_",
      "authorship_tag": "ABX9TyM2wwPNzXv6u2eCOX9EaPec"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}